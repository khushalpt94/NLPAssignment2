# -*- coding: utf-8 -*-
"""NLP_Assignment2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1bTdXtMek5bgeVLu15X3V8LVGRU8A3IfD
"""

# Commented out IPython magic to ensure Python compatibility.
# #importing the libraries that will help with the implementation
# 
# %%capture
# %pylab inline
# 
# import warnings
# warnings.filterwarnings('ignore')
# import csv
# import pandas as pd
# import numpy as np
# import seaborn as sns
# from sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer
# from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS
# from sklearn import metrics
# from sklearn.model_selection import train_test_split
# import matplotlib.pyplot as plt
# from sklearn.model_selection import cross_val_score
# from sklearn.svm import SVC
# from sklearn.linear_model import LogisticRegression
# from sklearn.naive_bayes import MultinomialNB
# from os import path
# from PIL import Image
# from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator
# 
# import pandas as pd
# 
# import seaborn as sns
# sns.set(style="white", rc={"axes.facecolor": (0, 0, 0, 0)})
# 
# from wordcloud import WordCloud, STOPWORDS
# from collections import Counter
# 
# np.random.seed(10)
# 
# from keras.utils import to_categorical
# import random
# from tensorflow import set_random_seed
# from sklearn.model_selection import train_test_split
# from keras.preprocessing import sequence
# from keras.preprocessing.text import Tokenizer
# from keras.layers import Dense,Dropout,Embedding,LSTM
# from keras.callbacks import EarlyStopping
# from keras.losses import categorical_crossentropy
# from keras.optimizers import Adam
# from keras.models import Sequential
# 
# #set random seed for the session and also for tensorflow that runs in background for keras
# set_random_seed(123)
# random.seed(123)

#Read the data from the link provided

URL_Tr ='https://raw.githubusercontent.com/cacoderquan/Sentiment-Analysis-on-the-Rotten-Tomatoes-movie-review-dataset/master/train.tsv'

df = pd.read_csv(URL_Tr,sep='\t')

df.count()

#total number of sentences

print('Total number of sentences : ' , len(df.SentenceId.unique()))

df.plot.scatter(x='PhraseId', y='SentenceId', title='Dataset')

# getting full sentences from the dataset
fullSent = df.loc[df.groupby('SentenceId')['PhraseId'].idxmin()]

fullSent.head()

first_df = st_df = df.groupby('SentenceId' , as_index = False).first()
first_df

#Depicting the bar plot of most data per sentiment
distance = df.groupby(["Sentiment"]).size()
distance = distance / distance.sum()
fig, ax = plt.subplots(figsize=(12,8))
sns.barplot(distance.keys(), distance.values);
fig.savefig('before.png')

#depiction of full sentences per sentiments 
distance = first_df.groupby(["Sentiment"]).size()
distance = distance / distance.sum()
fig, ax = plt.subplots(figsize=(12,8))
sns.barplot(distance.keys(), distance.values);
fig.savefig('AfterGraph.png')

# Start with one review:
text = df.Phrase[0]

# Create and generate a word cloud image:
wordcloud = WordCloud().generate(text)

# Display the generated image:
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis("off")
plt.show()

#downloading the stop words for english language
StopWords = ENGLISH_STOP_WORDS
print(StopWords)

text = " ".join(review for review in df.Phrase)
print ("There are {} words in the combination of all review.".format(len(text)))

# Create stopword list:
stopwords = set(StopWords)
stopwords.update(["drink", "now", "wine", "flavor", "flavors"])

# Generating a word cloud image
wordcloud = WordCloud(stopwords=stopwords, background_color="white").generate(text)

# Display the generated image:
# the matplotlib way:
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis("off")
plt.show()
plt.savefig('wordcloud.png')

#Performing Bag of word for the dataset
BOW_Vectorizer = CountVectorizer(strip_accents='unicode',
                                 stop_words=StopWords,
                                 ngram_range=(1,3),
                                 analyzer='word',
                                 min_df=5,
                                 max_df=0.5)

BOW_Vectorizer.fit(list(fullSent['Phrase']))

#create tfidf vectorizer 
tfidf_vectorizer = TfidfVectorizer(min_df=5,
                                 max_df=5,
                                  analyzer='word',
                                  strip_accents='unicode',
                                  ngram_range=(1,3),
                                  sublinear_tf=True,
                                  smooth_idf=True,
                                  use_idf=True,
                                  stop_words=StopWords)

tfidf_vectorizer.fit(list(fullSent['Phrase']))

#tfid
#build train dataset
phrase = fullSent['Phrase']
sentiment = fullSent['Sentiment']
phrase[0], sentiment[0]

#performing the cleansing operation on the dataset using lemmatizer from WordNetLemmatizer and SnowBallStemmer
from nltk.tokenize import word_tokenize
from nltk import FreqDist
from nltk.stem import SnowballStemmer,WordNetLemmatizer
stemmer=SnowballStemmer('english')
lemma=WordNetLemmatizer()
from string import punctuation
import re

def phrase_clean(review_col):
    review_corpus=[]
    for i in range(0,len(review_col)):
        review=str(review_col[i])
        review=re.sub('[^a-zA-Z]',' ',review)
        
        review=[lemma.lemmatize(w) for w in word_tokenize(str(review).lower())]
        review=' '.join(review)
        review_corpus.append(review)
    return review_corpus

import nltk
nltk.download('punkt')
nltk.download('wordnet')
df['phrase_clean']=phrase_clean(df.Phrase.values)

df.head()

from sklearn.utils import resample
train_2 = df[df['Sentiment']==2]
train_1 = df[df['Sentiment']==1]
train_3 = df[df['Sentiment']==3]
train_4 = df[df['Sentiment']==4]
train_5 = df[df['Sentiment']==0]
train_2_sample = resample(train_2,replace=True,n_samples=75000,random_state=2003)
train_1_sample = resample(train_1,replace=True,n_samples=75000,random_state=2003)
train_3_sample = resample(train_3,replace=True,n_samples=75000,random_state=2003)
train_4_sample = resample(train_4,replace=True,n_samples=75000,random_state=2003)
train_5_sample = resample(train_5,replace=True,n_samples=75000,random_state=2003)

df_upsampled = pd.concat([train_2, train_1_sample,train_3_sample,train_4_sample,train_5_sample])

df_upsampled.head()

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
# %matplotlib inline

from nltk.tokenize import TweetTokenizer
import datetime
#import lightgbm as lgb
from scipy import stats
from scipy.sparse import hstack, csr_matrix
from sklearn.model_selection import train_test_split, cross_val_score
#from wordcloud import WordCloud
from collections import Counter
from nltk.corpus import stopwords
from nltk.util import ngrams
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.naive_bayes import MultinomialNB
from sklearn.svm import LinearSVC
from sklearn.multiclass import OneVsRestClassifier

#data processing 

pd.set_option('max_colwidth',400)
text = ' '.join(df_upsampled.loc[df_upsampled.Sentiment == 4, 'Phrase'].values)
text_trigrams = [i for i in ngrams(text.split(), 3)]

Counter(text_trigrams).most_common(30)

tokenizer = TweetTokenizer()

vectorizer = TfidfVectorizer(ngram_range=(1, 2), tokenizer=tokenizer.tokenize)
full_text = list(df_upsampled['phrase_clean'].values)
vectorizer.fit(full_text)
df_upsampled_vectorized = vectorizer.transform(df_upsampled['phrase_clean'])

y = df_upsampled['Sentiment']

#to perfrom the vectorisation and bring the data in vector form
from keras.utils import to_categorical
X = df_upsampled['phrase_clean']
#Y = train['Sentiment']
Y = to_categorical(df_upsampled['Sentiment'].values)
print(Y)

from sklearn.model_selection import train_test_split
X_train, X_val, Y_train, Y_val = train_test_split(X, Y, test_size=0.3, random_state=2003)

print(X_train.shape,Y_train.shape)
print(X_val.shape,Y_val.shape)

from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences
from keras.utils import to_categorical

#joining the different tokens and finding total number of words
all_words=' '.join(X_train)
all_words=word_tokenize(all_words)

dist=FreqDist(all_words)

num_unique_word=len(dist)
num_unique_word
#X_train.head()

#finding the number of words for each phrase 
py_length=[]
for text in X_train:
    word=word_tokenize(text)
  #  print(text)
    l=len(word)
    py_length.append(l)
    
MAX_REVIEW_LEN=np.max(py_length)
MAX_REVIEW_LEN

max_features = num_unique_word
max_words = MAX_REVIEW_LEN
batch_size = 128
epochs = 5
num_classes=5

#defining the recall, precision and f1 score for the model
from keras import backend as K
def recall_m(y_true, y_pred):
  true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))
  possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))
  recall = true_positives / (possible_positives + K.epsilon())
  return recall

def precision_m(y_true, y_pred):
  true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))
  predicted_positives = K.sum(K.round(K.clip(y_true, 0, 1)))
  precision = true_positives / (predicted_positives + K.epsilon())
  return precision

def f1_m(y_true, y_pred):
  precision = precision_m(y_true, y_pred)
  recall = recall_m(y_true, y_pred)
  return 2*((precision*recall)/(precision+recall+K.epsilon()))

from tensorflow.python.keras.models import Sequential
from tensorflow.python.keras.layers import Dense, GRU, Embedding
from tensorflow.python.keras.optimizers import Adam
from tensorflow.python.keras.preprocessing.text import Tokenizer
from tensorflow.python.keras.preprocessing.sequence import pad_sequences

#tokenizing the words
tokenizer = Tokenizer(num_words=max_features)
tokenizer.fit_on_texts(X_train)
X_train = tokenizer.texts_to_sequences(X_train)
X_val = tokenizer.texts_to_sequences(X_val)

#X_test = tokenizer.texts_to_sequences(X_train)
#X_test

from keras.preprocessing import sequence,text
from keras.preprocessing.text import Tokenizer
from keras.models import Sequential
from keras.preprocessing.sequence import pad_sequences

#sequence padding for the model
from keras.preprocessing import sequence,text
from keras.preprocessing.text import Tokenizer
from keras.models import Sequential
from keras.preprocessing.sequence import pad_sequences
X_train = sequence.pad_sequences(X_train, maxlen=max_words)
X_val = sequence.pad_sequences(X_val, maxlen=max_words)
#X_test = sequence.pad_sequences(X_test, maxlen=max_words)
#print(X_train.shape,X_val.shape)
#X_test

from keras.preprocessing import sequence,text
from keras.preprocessing.text import Tokenizer
from keras.models import Sequential
from keras.layers import Dense,Dropout,Embedding,LSTM,Conv1D,GlobalMaxPooling1D,Flatten,MaxPooling1D,GRU,SpatialDropout1D,Bidirectional
from keras.callbacks import EarlyStopping
from keras.utils import to_categorical
from keras.losses import categorical_crossentropy
from keras.optimizers import Adam
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score,confusion_matrix,classification_report,f1_score
import matplotlib.pyplot as plt
from keras.layers import Input, Dense, Embedding, Flatten
from keras.layers import SpatialDropout1D
from keras.layers.convolutional import Conv1D, MaxPooling1D
from keras.layers.normalization import BatchNormalization
from keras.models import Sequential

model2 = Sequential()

# Input / Embdedding
model2.add(Embedding(max_features, 150, input_length=max_words))

# CNN
model2.add(SpatialDropout1D(0.2))

model2.add(Conv1D(64, kernel_size=3, padding='same', activation='relu'))
model2.add(MaxPooling1D(pool_size=2))

model2.add(BatchNormalization())
model2.add(Conv1D(32, kernel_size=3, padding='same', activation='relu'))
model2.add(MaxPooling1D(pool_size=2))

model2.add(Flatten())

# Output layer
model2.add(Dense(5, activation='softmax'))
model2.summary()

model2.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy', f1_m, precision_m, recall_m])
history = model2.fit(X_train, Y_train, validation_data=(X_val, Y_val), epochs=epochs, batch_size=batch_size, verbose=1)

def print_metrics(accuracy, f1_score, precision, recall):
  print('CNN MODEL PERFORMANCE')
  print('Accuracy: ', np.round(accuracy, 4))
  print('Precision: ', np.round(precision, 4))
  print('Recall: ', np.round(recall, 4))
  print('F1 Score: ', np.round(f1_score, 4))
  print('\n')

loss, accuracy, f1_score, precision, recall = model2.evaluate(X_val, Y_val)
print_metrics(accuracy, f1_score, precision, recall)

plt.plot(history.history['acc'])
plt.plot(history.history['val_acc'])
plt.title('Accuracy')
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.legend(['train', 'test'],fancybox=True, framealpha=1, shadow=True, borderpad=1, facecolor='white')
plt.savefig('accuracy.png')
#plt.show()
#plt.savefig('accuracy.png')

# Get training and test loss histories
training_loss = history.history['loss']
test_loss = history.history['val_loss']

# Create count of the number of epochs
epoch_count = range(1, len(training_loss) + 1)

# Visualize loss history
plt.plot(epoch_count, training_loss, 'r')
plt.plot(epoch_count, test_loss, 'b-')
plt.legend(['Training Loss', 'Test Loss'],fancybox=True, framealpha=1, shadow=True, borderpad=1, facecolor='white')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.savefig('actual_loss')
plt.show()

#saving model to disk
model2.save("1106937_1dconv_reg.h5")